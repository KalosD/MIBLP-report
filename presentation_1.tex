\documentclass{beamer}


%%%%% ===== 设置主题 *****
\usetheme{Berlin}
% 可供选择的主题参见 beameruserguide.pdf
% 无导航条的主题: Bergen, Boadilla, Madrid, CambridgeUS,
%                 GoettingenAnnArbor,Pittsburgh, Rochester;
% 有树形导航条的主题: Antibes, JuanLesPins, Montpellier;
% 有目录竖条的主题: Berkeley, PaloAlto, Goettingen, Marburg, Hannover;
% 有圆点导航条的主题: Berlin, Ilmenau, Dresden, Darmstadt, Frankfurt, Singapore, Szeged;
% 有节与小节导航条的主题: Copenhagen, Luebeck, Warsaw

\useinnertheme{circles}
\useoutertheme{infolines}
\usefonttheme[onlymath]{serif}
\setbeamertemplate{navigation symbols}{} % remove the navigation
\setbeamersize{text margin left=0.8cm, text margin right=0.8cm}
\setbeamerfont{frametitle}{size=\large}
\setbeamerfont{footline}{family=\ttfamily}
%%%%% ===== 宏包 *****
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx,xcolor}
\graphicspath{{figure/}}
\usepackage{hyperref}
\hypersetup{breaklinks=true}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{ulem}
% \usepackage{ctex}
% \usepackage{algorithmicx,algorithm}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{fontspec}
\usepackage{xeCJK}
%\usepackage{biblatex}
%\bibliographystyle{plain}
\usepackage[backend=bibtex,style=numeric,sorting=none]{biblatex}
%\addbibresource{example.bib} %BibTeX数据文件及位置
\addbibresource{bibtex3.bib}
\setbeamerfont{footnote}{size=\tiny}
%\usepackage[round，sort&compress]{natbib}
\renewcommand{\baselinestretch}{1.1}

%%%%% ===== 自定义命令 *****
\newcommand{\myem}[1]{\textcolor{blue}{#1}}
%\newtheorem{theorem}{Theorem}

\begin{document}
\title[最优潮流]%
{A Stochastic Conjugate Algorithms with the Minimized Variance Reduction
	\\}%An Adaptive Primal-Dual  Second-Order Corrector Algorithm  For Linear Programming
% \subtitle{subtitle if necessary}

\author[Hongbo Du]%
{Hongbo Du\\
Instructed by Caixia Kou\rule[0pt]{0pt}{20pt}\\}
%  \textcolor{red}{\texttt{xxxx@xxxx.xxx}}}

\institute[BUPT]{\textcolor[rgb]{0.0,0.0,0.10}%
{\small\ttfamily School of Science\\
Beijing University of Posts and Telecommunications\\[10pt]}}

\date{\today}

% ===== title page ====================================
\begin{frame}[plain]
	\titlepage
	%  Acknowledgement: part of this slides is based on Prof. Shiqian Ma and
	% Prof. Stephen P. Boyd’s lecture notes
\end{frame}

\begin{frame}
	\frametitle{Outline}
	\tableofcontents[hideallsubsections] %[pausesections]
\end{frame}

\AtBeginSection[] % Do nothing for \section*
{ \begin{frame}<beamer> %\frametitle{Outline}
		\tableofcontents[currentsection,hideallsubsections]%,currentsubsection]
	\end{frame}
}

%===== Main part start here ==========================
%===== 开题报告 =======================================




\section{Introduction}

\begin{frame}
	\frametitle{\bf Introduction} 
	\textbf { The research problem }
	$$\min\limits_{\omega \in R^{d}} f(\omega) = \frac{1}{n} \sum_{i=1}^n f_i(\omega)$$\\
	\textbf {Linear regression（Ridge regression）}
	$$\min\limits_{\omega \in R^{d}} f(\omega) =\frac{1}{n} \sum_{i=1}^n(x_i^T \omega - y_i)^2 + \frac{\lambda}{2}\Vert \omega \Vert ^2= \frac{1}{n} \sum_{i=1}^n f_i(\omega)$$
	$$f_i(\omega)=  (x_i^T \omega - y_i)^2 + \frac{\lambda}{2}\Vert \omega \Vert ^2$$\\
	\textbf { Logistic regression  }
	$$\min\limits_{\omega \in R^{d}} f(\omega) =\frac{1}{n} \sum_{i=1}^n log(1+exp(-y_i x_i^T \omega)) + \frac{\lambda}{2}\Vert \omega \Vert ^2 $$
$$f_i(\omega)= log(1+exp(-y_i x_i^T \omega)) + \frac{\lambda}{2}\Vert \omega \Vert ^2	$$\\
\end{frame}

\begin{frame}
	\frametitle{\bf Introduction} 
	\textbf { GD(Gradient Decent)}
	$$\omega_{k+1}=\omega_k - \alpha_k \nabla f(\omega_k)$$
	$$ \nabla f(\omega_k) = \frac{1}{n} \sum_{i=1}^n\nabla f_i(\omega_k) $$
	\textbf { SGD(Robbin, Monro, 1951)}  
	$$\omega_{k+1}=\omega_k - \alpha_k \nabla f_i(\omega_k) $$
    \textbf { Mini-batch SGD (Shalev-Shwartz et al., 2007)} \\
    Fixed stepsize：$$\omega_{k+1}=\omega_k - \alpha \nabla f_{S_k}(\omega_k)$$
    Decreased stepsize：$$\omega_{k+1}=\omega_k - \alpha_k \nabla f_{S_k}(\omega_k) $$
    $\text{where }\nabla f_{S_k}(\omega_k)=\frac{1}{|S|} \sum_{i\in S_k} \nabla f_i(\omega_k)$
\end{frame}

\begin{frame}
	\frametitle{\bf The Stochastic Gradient Algorithm with Variance Reduction} 
			\textbf { SVRG(R. Johnson and T. Zhang, 2013)}
$$g_k = \nabla f_j(\omega^k)-\nabla f_j(\psi_j^k)+ \frac{1}{n} \sum_{i=1}^{n}\nabla f_i(\psi_i^k)$$
$$g_k = \nabla f_{S_k}(\omega^k)-\nabla f_{S_k}(\psi_j^k)+ \frac{1}{n} \sum_{i=1}^{n}\nabla f_i(\psi_i^k)$$
			\textbf { SAGA(A. Defazio, F. Bach, and S. Lacoste-Julien, 2014)}
	$$g_k = \nabla f_{S_k}(\omega^k)-\nabla f_{S_k}(\phi_j^k)+ \frac{1}{n} \sum_{i=1}^{n}\nabla f_i(\phi_i^k)$$
\textbf { SAG (M. Schmidt, N. Le Roux, and F. Bach, 2017)}
$$g_k = \frac{\nabla f_{S_k}(\omega^k)-\nabla f_{S_k}(\phi_j^k)}{n} + \frac{1}{n}\sum_{i=1}^{n}\nabla f_i(\phi_i^k)$$
\end{frame}

\section{The Minimal Variance Stochastic Gradient Estimate}

\begin{frame}
	\frametitle{\bf Motivation} 
    Let $$\bar{X} = \nabla f_{S_k}(\omega^k), \bar{Y} = \nabla f_{S_k}(\phi_i^k), or \bar{Y} = \nabla f_{S_k}(\psi_i^k)$$\\
   \textbf { (SAGA/SVRG)}
    $$g_k =\bar{X}-\bar{Y}+E(\bar{Y}) $$
    $$E(g_k) = E(\bar{X}-\bar{Y}+E(\bar{Y})) = E(\bar{X})$$
    $$Var(g_k) = Var(\bar{X}-\bar{Y}+E(\bar{Y}))=Var(\bar{X}-\bar{Y})$$
    \textbf { (SAG)}
    $$g_k = \frac{\bar{X}-\bar{Y}}{n}+E(\bar{Y}) $$
	$$E(g_k) = E(\frac{\bar{X}-Y}{n}+E(\bar{Y})) = \frac{1}{n}E(\bar{X})+(1-\frac{1}{n})E(\bar{Y})$$
	$$Var(g_k) = Var(\frac{\bar{X}-\bar{Y}}{n}+E(\bar{Y}))= \frac{1}{n^2} Var(\bar{X}-\bar{Y})$$
\end{frame}

\begin{frame}
	\frametitle{\bf Gradient Estimate with Unbiasedness and Minimal Variance} 
	\textbf { Estimator}：$$\theta_{\gamma}=\bar{X}-\gamma(\bar{Y}-E(\bar{Y}))$$ \\
	\textbf { Expected Value}：$$E(\theta_{\gamma})=E(\bar{X})$$
	\textbf { Variance}：\begin{equation}
		\begin{aligned}
			Var(\theta_{\gamma}) & =Var(\bar{X}-\gamma(\bar{Y}-E(\bar{Y})))\\
			& =Var(\bar{X})+\gamma^2 Var(\bar{Y})-2 \gamma Cov(\bar{X},\bar{Y})
			\label{2:variance}
		\end{aligned}
	\end{equation}
	\textbf { Best Param}：
\begin{equation}
		\gamma^*=\frac{Cov(\bar{X}, \bar{Y})}{Var(\bar{Y})}\approx \frac{s_{XY}}{s_Y^2}
			\label{3:gamma_star}
\end{equation}
\begin{equation}
	\begin{aligned}
		s_{XY}&=\frac{1}{|S|-1}\sum_{j\in S_k}(X_j-\bar{X})(Y_j-\bar{Y})\\
		s_{Y}^2&=\frac{1}{|S|-1}\sum_{j\in S_k}(Y_j-\bar{Y})^2
		\label{2:sample_variance}
	\end{aligned}
\end{equation}

\end{frame}

\begin{frame}
	\frametitle{\bf Estimation of $\gamma^*$} 
\begin{equation}
	\begin{aligned}
		Cov(\bar{X},\bar{Y})  &=Cov(\frac{1}{|S|}\sum_{j\in S_k} X_j,\frac{1}{|S|}\sum_{j\in S_k} Y_j)=\frac{1}{|S|^2}Cov(\sum_{j\in S_k} X_j,\sum_{j\in S_k} Y_j)\\
		&=\frac{1}{|S|^2}\sum_{j\in S_k} Cov(X_j,Y_j)=\frac{1}{|S|} Cov(X,Y)\approx \frac{1}{|S|}s_{XY}
		\label{2:cov_estimate}
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
		Var(\bar{Y})  &=Var(\frac{1}{|S|}\sum_{j\in S_k} Y_j)=\frac{1}{|S|^2}Var(\sum_{j\in S_k} Y_j)\\&=\frac{1}{|S|^2}\sum_{j\in S_k} Var(Y_j)		=\frac{1}{|S|} Var(Y)\approx \frac{1}{|S|}s_{Y}^2
		\label{2:var_estimate}
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
		s_{XY}&=\frac{1}{|S|-1}\sum_{j\in S_k}(X_j-\bar{X})(Y_j-\bar{Y})\\
		s_{Y}^2&=\frac{1}{|S|-1}\sum_{j\in S_k}(Y_j-\bar{Y})^2
		\label{2:sample_variance}
	\end{aligned}
\end{equation}
\end{frame}
\begin{frame}
	\frametitle{\bf The Stochastic Conjugate Gradient Algorithm with Variance Reduction} 
			\textbf { CGVR(Ran Xin, Usman A. Khan, and Soummya Kar, 2020)}
$$g_k = \nabla f_{S_k}(\omega^k)-\nabla f_{S_k}(\psi_j^k)+ \frac{1}{n} \sum_{i=1}^{n}\nabla f_i(\psi_i^k)$$
$$d_k = -g_k +\beta_k d_{k-1}$$
			\textbf { SCGA(Caixia Kou and Han Yang, 2022)}
	$$g_k = \nabla f_{S_k}(\omega^k)-\nabla f_{S_k}(\phi_j^k)+ \frac{1}{n} \sum_{i=1}^{n}\nabla f_i(\phi_i^k)$$
    $$d_k = -g_k +\beta_k d_{k-1}$$
\end{frame}

\section{Algorithm and Convergence analysis}
\begin{frame}
\begin{algorithm}[H]
	\DontPrintSemicolon
	\textbf{Initialization:} .....\\
	\textbf{Iteration:}\\
	\For {$k = 1,2...$} 
	{
		......\\
		\For {$j : S_k$}
		{
			Compute $\nabla f_{j}(\omega_k)$ and store it into matrix $\nabla f_{[S_k]}(\omega_k) $\\
			Select $\nabla f_{j}(\omega_{[k-1]})$ in $\nabla f(\omega_{[k-1]})$ and store it into matrix $\nabla f_{[S_k]}(\omega_{[k-1]})$\\
		}
		\For {$r = 1,2...d$}
		{
			Compute \textcolor{red}{the sample covariance} of $\nabla f_{[S_k]}^{(r)}(\omega_k)$ and $\nabla f_{[S_k]}^{(r)}(\omega_{[k-1]})$, \textcolor{red}{the sample variance} of $\nabla f_{[S_k]}^{(r)}(\omega_{[k-1]})$ using  (\ref{2:sample_variance})\\
			Compute \textcolor{red}{$\gamma^{*(r)}$} using (\ref{3:gamma_star})
		}
		Compute $\nabla f_{S_k}(\omega_k)=\frac{1}{|S|}  \sum_{j\in S_k}\nabla f_{j}(\omega_k),\mu_{S_k}=\frac{1}{|S|} \sum_{j\in S_k}\nabla f_{j}(\omega_{[k-1]})$ \\			
		Compute $g_k =\nabla f_{S_k}(\omega_k)-\textcolor{red}{\gamma^*} (\mu_{S_k}- \mu_{k-1})$\\
		......\\
	}
	\caption{SCGA with the minimal variance stochastic gradient estimate}
\end{algorithm}
\end{frame}
\begin{frame}
	\begin{algorithm}[H]
		\DontPrintSemicolon
		\textbf{Initialization:} Given $x_0 \in R^d$, compute $h_0 = \frac{1}{n} \sum_{i=1}^{n} \nabla f_i(x_0)$:\\
		\textbf{Iteration:}		
		\For {$l = 1,2...T$} 
		{
			......\\
			\For {$k = 1,2...m$} 
			{
				......\\
				\For {$j : S_{k}$}
				{
					Compute $\nabla f_{j}(\omega_k),\nabla f_{j}(\omega_0)$\\
					Store $\nabla f_{[S_k]}(\omega_k)\leftarrow \nabla f_{j}(\omega_k),\nabla f_{[S_k]}(\omega_{0})\leftarrow \nabla f_{j}(\omega_0)$\\
				}
				\For {$r = 1,2...d$}
				{
					Compute \textcolor{red}{the sample covariance} $\nabla f_{[S_k]}^{(r)}(\omega_k)$ and $\nabla f_{[S_k]}^{(r)}(\omega_0)$,\textcolor{red}{ the sample variance} of $\nabla f_{[S_k]}^{(r)}(\omega_0)$ using  (\ref{2:sample_variance})\\
					Compute \textcolor{red}{$\gamma^{*(r)}$} using (\ref{3:gamma_star})
				}
				Compute $\nabla f_{S_k}(\omega_k),\nabla f_{S_k}(\omega_0)$ \\			
				Compute $g_k =\nabla f_{S_k}(\omega_k)-\textcolor{red}{\gamma^*} (\nabla f_{S_k}(\omega_0)- \mu_{l-1})$\\
				......\\		
			}
			......
		}
		\caption{CGVR with the minimal variance stochastic gradient estimate}
	\end{algorithm}
\end{frame}

\begin{frame}
		\frametitle{\bf Assumptions} 
	\textbf{Assumption 1($\mu$ -strong convexity and $L$  -smoothness)}  $f_i,1 \leq i \leq n$ is strongly convex and has Lipschitz continuous gradients, i.e.,
	
	\begin{equation}
		\mu I \prec \nabla^2 f_i(w)\prec LI 
		\label{4:assumptinon_1}
	\end{equation}
	For $\omega \in R^d$, $\mu$ is strong convexity constant and $L$ is Lipschitz constant.\\
	\textbf{Assumption 2 (low and upper bounds of step size)} every step size $\alpha_k$  in Alg1 and Alg2 algorithm satisfies  $\alpha_1 \leq \alpha_k \leq \alpha_2$\\
	\textbf{Assumption 3 (upper bound of scalar $\beta_k$ )} There exists constants $\beta$  such that 
	\begin{equation}
		\beta_k \leq \frac{\|g_k\|^2}{\|g_{k-1}\|^2} \leq \beta
		\label{4:assumption_3}
	\end{equation} 
\end{frame}

\begin{frame}
		\frametitle{\bf Related Lemma} 
	\begin{lemma}  
		Under Assumption1, we have
		\begin{equation}
			2\mu(f(\omega)-f(\omega^*)) \leq \|\nabla f(w)\|^2 \leq 2L(f(\omega)-f(\omega^*))
			\label{4:lemma_1_formula}
		\end{equation}
		Where $\omega \in R^d$, $\omega^*$ is the unique minimizer
	\end{lemma}
	\begin{lemma} 
		Consider that Alg1 and Alg2 (CG) algorithm, where step size $\alpha_k$ satisfies strong Wolfe condition with $0 < \sigma_2 < \frac{1}{2}$ and $\beta_k$  satisfies $|\beta_k| \leq \beta_k^{FR}$ , then it generates descent directions $d_k$ satisfying
		\begin{equation}
			-\frac{1}{1-\sigma_2} \leq \frac{\langle g_k,d_k \rangle}{\|g_k\|^2} \leq \frac{2\sigma_2-1}{1-\sigma_2}
			\label{4:lemma_2_formula}
		\end{equation} 
	\end{lemma}
\end{frame}
\begin{frame}
	\frametitle{\bf Convergence of Alg1} 
\begin{theorem}  
	Let Assumption 1,2,3 hold. If the bound of the step-size in Alg1 satisfies:
	
	\begin{equation}
		0 < \alpha_1 < \frac{1-\beta}{2L\sigma_1}
		\label{4:alpha_between}
	\end{equation} 
	Then we have:  $\forall k>0$
	
	\begin{equation}
		E(f(\omega_k))-f({\omega^*}) \leq C\xi^k (E(f(\omega_0))-f(\omega^*))
		\label{4:theorem_concludtion}
	\end{equation} 
	where $\xi = \frac{(1-\sigma_1)(1-\beta)+2L\alpha\sigma_1\sigma_2(1-\beta^m)}{2\mu\sigma_1m(1-\sigma_2)(1-\beta)}<1, \omega^*$ is the unique minimizer of $f$
\end{theorem}
\end{frame}

\begin{frame}
	\frametitle{\bf Convergence of Alg2} 
\begin{theorem}
	Let Assumption 1,2,3 hold. Let $$m>\frac{(1-\sigma_1)+2L\alpha_2\sigma_1\sigma_2\beta}{2\mu\sigma_1\alpha_1(1-\sigma_2)}$$
	Then we have:  $\forall l>0$
	
	\begin{equation}
		E(f(x_l))-f({\omega^*}) \leq \xi^l (E(f(x_0))-f(\omega^*))
		\label{4:theorem2_conludtion}
	\end{equation} 
	where $$\xi = \frac{(1-\sigma_1)(1-\beta)+2L\alpha\sigma_1\sigma_2(1-\beta^m)}{2\mu\sigma_1m(1-\sigma_2)(1-\beta)}<1$$ and $ \omega^*$   is the unique minimizer of $f$
\end{theorem}
\end{frame}
\section{Numerical Experiments}
\begin{frame}
	\frametitle{\bf Datasets} 
\begin{center}
	\begin{table}
		\centering
		\caption{Summary of data sets used in numerical experiments}
		\label{tab:1}       % Give a unique label
		% For LaTeX tables use
		\begin{tabular}{l c c l}
			\hline\noalign{\smallskip}
			dataset & d & n & type \\
			\noalign{\smallskip}\hline\noalign{\smallskip}
			A9a & 123 & 32561&binary classification  \\
			Ijcnn1 & 22 & 49990 &binary classification  \\
			Protein & 74 & 145751&binary classification  \\
			Quantum & 78 & 50000&binary classification  \\
			W8a & 300 & 49749&binary classification  \\
			Covtype & 54 & 581012 &binary classification  \\
			YearPredictionMSD & 90 & 463715 &regression \\
			Pyrim & 27 & 74 &regression \\
			Bodyfat & 24 & 252&regression  \\
			Triazines & 60 & 180&regression  \\
			Eunite2001 & 16 & 336&regression  \\
			Cpusmall & 12 & 8192&regression  \\
			\noalign{\smallskip}\hline
		\end{tabular}
	\end{table}
\end{center}
\end{frame}

\begin{frame}
	\frametitle{\bf Test Function} 
The ridge regression model are presented as follows:
\begin{equation}
	\begin{aligned}
		\min\limits_{\omega} \frac{1}{n} \sum_{i=1}^{n} (y_i -x_i \omega)^2 +\lambda \|\omega\|^2
		\label{5:ridge}
	\end{aligned}
\end{equation} 
where $x_i \in R^d$ is denoted the feature vector of the i-th  data sample, $y_i \in R$  is denoted the actual value of the i-th data sample, and $\lambda$  is the regularization parameter.
\end{frame}
\begin{frame}
	\frametitle{\bf Numerical Results} 
\begin{figure}
	\begin{center}
		% \includegraphics[width=0.5\textwidth]{variance.eps}
	\end{center}
	\caption{Variance comparison of stochastic gradient estimates: $g_{S_l}^k(\gamma=\gamma^*)$ and $g_{S_l}^k(\gamma=1)$}
	\label{fig:1}       % Give a unique label
\end{figure}
\begin{equation}
	\begin{aligned}
		g_{S_l}^k(\gamma) &= \nabla f_{S_l}(w_{101})-\gamma(\nabla f_{S_l}(w_{k})-\frac{1}{n}\sum_{i=1}^n \nabla f_i(w_k))
		l=1,...100
		\label{5:experiment_gradient}
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
		Var(g_{S_l}^k(\gamma))=\frac{1}{100}\sum_{l=1}^{100}(g_{S_l}^k(\gamma)-\frac{1}{100}\sum_{l=1}^{100}g_{S_l}^k(\gamma))^2, k=0,1,2...100
		\label{5:experiment_variance}
	\end{aligned}
\end{equation}
\end{frame}

\begin{frame}
	\frametitle{\bf Numerical Results} 

\begin{figure}
	\begin{center}
		% \includegraphics[width=0.95\textwidth]{total1.eps}
	\end{center}
	\caption{performance profiles of SCGA,CGVR,Alg1,Alg2 on the six data sets of binary classification (x-axis is times of iteration, y-axis is loss value in terms of log10)}
	\label{fig:2}       % Give a unique label
\end{figure}
\end{frame}

\begin{frame}
	\frametitle{\bf Numerical Results} 
\begin{figure}
	\begin{center}
		% \includegraphics[width=0.95\textwidth]{total2.eps}
	\end{center}
	\caption{performance profiles of SCGA,CGVR,Alg1,Alg2 on the six data sets of regression (x-axis is times of iteration, y-axis is loss value in terms of log10)}
	\label{fig:3}       % Give a unique label
\end{figure}
\end{frame}
\section{Conclusion}
\begin{frame}
	\frametitle{\bf Conclusion} 
			    \begin{enumerate}[(1)]
		\item Propose the minimal variance stochastic gradient estimate \\
	\item Propose two improved algorithms of SCGA and CGVR: Alg1 and Alg2
	\item Prove the linear convergence rate of the new algorithms under strong convexity and smoothness
	\item From a series of experiments, compared with SCGA and CGVR, Alg1 and Alg2 is competitive algorithms.
\end{enumerate}
\end{frame}
\begin{frame}
\vspace{1em}
\vspace{1em}
\vspace{1em}
\vspace{1em}
\vspace{1em}
\vspace{1em}
\vspace{1em}
\vspace{1em}
\vspace{1em}
\vspace{1em}
\centering
\textcolor{blue}{\LARGE Thanks for your attention!}

\end{frame}

\end{document}
